{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2526207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# models\n",
    "# from lightgbm import LGBMClassifier\n",
    "# BDT = LGBMClassifier()\n",
    "\n",
    "import nltk \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f5f148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cap-shape_b  cap-shape_c  cap-shape_f  cap-shape_k  cap-shape_s  \\\n",
      "0           False        False        False        False        False   \n",
      "1           False        False        False        False        False   \n",
      "2            True        False        False        False        False   \n",
      "3           False        False        False        False        False   \n",
      "4           False        False        False        False        False   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "8119        False        False        False         True        False   \n",
      "8120        False        False        False        False        False   \n",
      "8121        False        False         True        False        False   \n",
      "8122        False        False        False         True        False   \n",
      "8123        False        False        False        False        False   \n",
      "\n",
      "      cap-shape_x  cap-surface_f  cap-surface_g  cap-surface_s  cap-surface_y  \\\n",
      "0            True          False          False           True          False   \n",
      "1            True          False          False           True          False   \n",
      "2           False          False          False           True          False   \n",
      "3            True          False          False          False           True   \n",
      "4            True          False          False           True          False   \n",
      "...           ...            ...            ...            ...            ...   \n",
      "8119        False          False          False           True          False   \n",
      "8120         True          False          False           True          False   \n",
      "8121        False          False          False           True          False   \n",
      "8122        False          False          False          False           True   \n",
      "8123         True          False          False           True          False   \n",
      "\n",
      "      ...  population_s  population_v  population_y  habitat_d  habitat_g  \\\n",
      "0     ...          True         False         False      False      False   \n",
      "1     ...         False         False         False      False       True   \n",
      "2     ...         False         False         False      False      False   \n",
      "3     ...          True         False         False      False      False   \n",
      "4     ...         False         False         False      False       True   \n",
      "...   ...           ...           ...           ...        ...        ...   \n",
      "8119  ...         False         False         False      False      False   \n",
      "8120  ...         False          True         False      False      False   \n",
      "8121  ...         False         False         False      False      False   \n",
      "8122  ...         False          True         False      False      False   \n",
      "8123  ...         False         False         False      False      False   \n",
      "\n",
      "      habitat_l  habitat_m  habitat_p  habitat_u  habitat_w  \n",
      "0         False      False      False       True      False  \n",
      "1         False      False      False      False      False  \n",
      "2         False       True      False      False      False  \n",
      "3         False      False      False       True      False  \n",
      "4         False      False      False      False      False  \n",
      "...         ...        ...        ...        ...        ...  \n",
      "8119       True      False      False      False      False  \n",
      "8120       True      False      False      False      False  \n",
      "8121       True      False      False      False      False  \n",
      "8122       True      False      False      False      False  \n",
      "8123       True      False      False      False      False  \n",
      "\n",
      "[8124 rows x 112 columns]\n",
      "[1 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# import dataset 1 (Mushroom)\n",
    "  \n",
    "# fetch dataset \n",
    "mushroom = fetch_ucirepo(id=73) \n",
    "      \n",
    "# data (as pandas dataframes) \n",
    "data_mushroom = mushroom.data.features\n",
    "targets_mushroom = mushroom.data.targets\n",
    "\n",
    "# metadata \n",
    "# print(mushroom.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(mushroom.variables) \n",
    "\n",
    "# poisonous = 1, edible = 0\n",
    "label_encoder = LabelEncoder()\n",
    "targets_mushroom = label_encoder.fit_transform(targets_mushroom)\n",
    "\n",
    "# remove a column that has NaN values\n",
    "data_mushroom = data_mushroom.drop(columns='stalk-root', axis=1)\n",
    "\n",
    "#categorical_cols = [col for col in data_mushroom.columns if data_mushroom[col].dtype == '0']\n",
    "\n",
    "#preprocessor = ColumnTransformer(\n",
    "#    transformers=[\n",
    "#        ('cat', OneHotEncoder(), categorical_cols)\n",
    "#    ],\n",
    "#    remainder='passthrough'\n",
    "#)\n",
    "\n",
    "#data_mushroom = preprocessor.fit_transform(data_mushroom)\n",
    "\n",
    "data_mushroom = pd.get_dummies(data_mushroom)\n",
    "\n",
    "print(data_mushroom)\n",
    "print(targets_mushroom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b44e3159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Atr1  Atr2  Atr3  Atr4  Atr5  Atr6  Atr7  Atr8  Atr9  Atr10  ...  Atr45  \\\n",
      "0       2     2     4     1     0     0     0     0     0      0  ...      3   \n",
      "1       4     4     4     4     4     0     0     4     4      4  ...      2   \n",
      "2       2     2     2     2     1     3     2     1     1      2  ...      2   \n",
      "3       3     2     3     2     3     3     3     3     3      3  ...      3   \n",
      "4       2     2     1     1     1     1     0     0     0      0  ...      2   \n",
      "..    ...   ...   ...   ...   ...   ...   ...   ...   ...    ...  ...    ...   \n",
      "165     0     0     0     0     0     0     0     0     0      0  ...      0   \n",
      "166     0     0     0     0     0     0     0     0     0      0  ...      3   \n",
      "167     1     1     0     0     0     0     0     0     0      1  ...      2   \n",
      "168     0     0     0     0     0     0     0     0     0      0  ...      4   \n",
      "169     0     0     0     0     0     0     0     1     0      0  ...      1   \n",
      "\n",
      "     Atr46  Atr47  Atr48  Atr49  Atr50  Atr51  Atr52  Atr53  Atr54  \n",
      "0        2      1      3      3      3      2      3      2      1  \n",
      "1        2      2      3      4      4      4      4      2      2  \n",
      "2        3      2      3      1      1      1      2      2      2  \n",
      "3        2      2      3      3      3      3      2      2      2  \n",
      "4        2      1      2      3      2      2      2      1      0  \n",
      "..     ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
      "165      1      0      4      1      1      4      2      2      2  \n",
      "166      4      1      2      2      2      2      3      2      2  \n",
      "167      3      0      2      0      1      1      3      0      0  \n",
      "168      3      3      2      2      3      2      4      3      1  \n",
      "169      3      4      4      0      1      3      3      3      1  \n",
      "\n",
      "[170 rows x 54 columns]\n",
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "165    0\n",
      "166    0\n",
      "167    0\n",
      "168    0\n",
      "169    0\n",
      "Name: Class, Length: 170, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# import dataset 3 (Divorce Prediction)\n",
    "\n",
    "# Atr1 - Atr 54 : questions with answers with ranges [0-4] (0=Never, 1=Seldom, 2=Averagely, 3=Frequently, 4=Always)\n",
    "# Class : 1 is divorced, 0 is married\n",
    "data_divorce = pd.read_csv(\"divorce.csv\", sep=';', engine=\"python\", na_values=\"?\")\n",
    "\n",
    "# separate feature set from target\n",
    "targets_divorce = data_divorce[\"Class\"]\n",
    "data_divorce = data_divorce.drop(columns='Class', axis=1)\n",
    "\n",
    "print(data_divorce)\n",
    "print(targets_divorce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4890df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset 2 (Gender)\n",
    "\n",
    "# Name, Gender (M, F), Count, Probability\n",
    "# data_gender = pd.read_csv(\"name_gender_dataset.csv\", engine=\"python\",na_values=\"?\")\n",
    "\n",
    "# n-gram analysis, https://arxiv.org/pdf/2102.03692.pdf, Yifan Hu et al.\n",
    "# def last_char(name): \n",
    "#    if (len(name) == 0):\n",
    "#        return \"\"\n",
    "#    return name[-1]\n",
    "\n",
    "#def last_two_char(name):\n",
    "#    if (len(name) < 2):\n",
    "#        return last_char(name)\n",
    "#    return name[-2:]\n",
    "\n",
    "#def last_three_char(name):\n",
    "#    if (len(name) < 3):\n",
    "#        return last_two_char(name)\n",
    "#    return name[-3:]\n",
    "\n",
    "#add_features = pd.DataFrame(last_two_char(data_gender[\"Name\"][i]) for i in data_gender.index)\n",
    "\n",
    "# add_features.columns = [\"one_gram\", \"two_gram\", \"three_gram\"]\n",
    "#add_features.columns = [\"two_gram\"]\n",
    "\n",
    "# print(add_features)\n",
    "\n",
    "# added n-grams of last characters, n = 1, 2, 3\n",
    "#data_gender = data_gender.join(add_features)\n",
    "\n",
    "#targets_gender = data_gender[\"Gender\"]\n",
    "#data_gender = data_gender.drop(columns='Gender', axis=1)\n",
    "\n",
    "#word_vec_length = data_gender[\"Name\"].apply(len).max()\n",
    "#accepted_chars = 'abcdefghijklmnopqrstuvwxyz'\n",
    "#char_vec_length = len(accepted_chars)\n",
    "\n",
    "# Define a mapping of chars to integers\n",
    "#char_to_int = dict((c, i) for i, c in enumerate(accepted_chars))\n",
    "#int_to_char = dict((i, c) for i, c in enumerate(accepted_chars))\n",
    "\n",
    "# Returns a list of n lists with n = word_vec_length\n",
    "#def name_encoding(name):\n",
    "\n",
    "    # Encode input data to int, e.g. a->1, z->26\n",
    "#    integer_encoded = [char_to_int[char] for i, char in enumerate(name)]\n",
    "    \n",
    "    # Start one-hot-encoding\n",
    "#    onehot_encoded = list()\n",
    "    \n",
    "#    for value in integer_encoded:\n",
    "#        # create a list of n zeros, where n is equal to the number of accepted characters\n",
    "#        letter = [0 for _ in range(char_vec_length)]\n",
    "#        letter[value] = 1\n",
    "#        onehot_encoded.append(letter)\n",
    "        \n",
    "    # Fill up list to the max length. Lists need do have equal length to be able to convert it into an array\n",
    "#    for _ in range(word_vec_length - len(name)):\n",
    "#        onehot_encoded.append([0 for _ in range(char_vec_length)])\n",
    "#        \n",
    "#    return onehot_encoded\n",
    "\n",
    "\n",
    "# data_gender = pd.get_dummies(data_gender, columns=\"two_gram\")\n",
    "\n",
    "# print(data_gender.head())\n",
    "# print(targets_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd53c61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0     63    1   1       145   233    1        2      150      0      2.3   \n",
      "1     67    1   4       160   286    0        2      108      1      1.5   \n",
      "2     67    1   4       120   229    0        2      129      1      2.6   \n",
      "3     37    1   3       130   250    0        0      187      0      3.5   \n",
      "4     41    0   2       130   204    0        2      172      0      1.4   \n",
      "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
      "297   57    0   4       140   241    0        0      123      1      0.2   \n",
      "298   45    1   1       110   264    0        0      132      0      1.2   \n",
      "299   68    1   4       144   193    1        0      141      0      3.4   \n",
      "300   57    1   4       130   131    0        0      115      1      1.2   \n",
      "301   57    0   2       130   236    0        2      174      0      0.0   \n",
      "\n",
      "     slope   ca  thal  \n",
      "0        3  0.0   6.0  \n",
      "1        2  3.0   3.0  \n",
      "2        2  2.0   7.0  \n",
      "3        3  0.0   3.0  \n",
      "4        1  0.0   3.0  \n",
      "..     ...  ...   ...  \n",
      "297      2  0.0   7.0  \n",
      "298      2  0.0   7.0  \n",
      "299      2  2.0   7.0  \n",
      "300      2  1.0   7.0  \n",
      "301      2  1.0   3.0  \n",
      "\n",
      "[297 rows x 13 columns]\n",
      "     num\n",
      "0      0\n",
      "1      1\n",
      "2      1\n",
      "3      0\n",
      "4      0\n",
      "..   ...\n",
      "297    1\n",
      "298    1\n",
      "299    1\n",
      "300    1\n",
      "301    1\n",
      "\n",
      "[297 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "data_hd = heart_disease.data.features \n",
    "targets_hd = heart_disease.data.targets \n",
    "  \n",
    "# metadata \n",
    "# print(heart_disease.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(heart_disease.variables) \n",
    "\n",
    "# convert to binary classification\n",
    "for i in targets_hd.index:\n",
    "    if targets_hd[\"num\"][i] > 0:\n",
    "        targets_hd[\"num\"][i] = 1\n",
    "\n",
    "# remove NaN values\n",
    "data_hd = data_hd.join(targets_hd).dropna()\n",
    "targets_hd = pd.DataFrame(data_hd[\"num\"])\n",
    "data_hd = data_hd.drop(columns=[\"num\"])\n",
    "        \n",
    "print(data_hd)\n",
    "print(targets_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88338c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle datasets\n",
    "\n",
    "#np.random.shuffle(data_mushroom)\n",
    "#np.random.shuffle(data_gender)\n",
    "#np.random.shuffle(data_divorce)\n",
    "\n",
    "# df_m = data_mushroom.sample(frac=1).reset_index(drop=True)\n",
    "# df_g = data_gender.sample(frac=1).reset_index(drop=True)\n",
    "# df_d = data_divorce.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# m_target = df_m['poisonous']\n",
    "\n",
    "# print(df_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b90615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split train and test sets\n",
    "\n",
    "split_seed = 22\n",
    "\n",
    "# Mushroom partition\n",
    "mX_train_80, mX_test_20, my_train_80, my_test_20 = train_test_split(data_mushroom, targets_mushroom, train_size=0.8, \n",
    "                                                        random_state=split_seed, shuffle=True)\n",
    "\n",
    "mX_train_50, mX_test_50, my_train_50, my_test_50 = train_test_split(data_mushroom, targets_mushroom, train_size=0.5, \n",
    "                                                        random_state=split_seed, shuffle=True)\n",
    "\n",
    "mX_train_20, mX_test_80, my_train_20, my_test_80 = train_test_split(data_mushroom, targets_mushroom, train_size=0.2, \n",
    "                                                        random_state=split_seed, shuffle=True)\n",
    "\n",
    "# Divorce partition\n",
    "dX_train_80, dX_test_20, dy_train_80, dy_test_20 = train_test_split(data_divorce, targets_divorce, train_size=0.8, \n",
    "                                                        random_state=split_seed, shuffle=True)\n",
    "\n",
    "dX_train_50, dX_test_50, dy_train_50, dy_test_50 = train_test_split(data_divorce, targets_divorce, train_size=0.5, \n",
    "                                                        random_state=split_seed, shuffle=True)\n",
    "\n",
    "dX_train_20, dX_test_80, dy_train_20, dy_test_80 = train_test_split(data_divorce, targets_divorce, train_size=0.2, \n",
    "                                                        random_state=split_seed, shuffle=True)\n",
    "\n",
    "\n",
    "# Heart Disease partition\n",
    "hX_train_80, hX_test_20, hy_train_80, hy_test_20 = train_test_split(data_hd, targets_hd, train_size=0.8, \n",
    "                                                        random_state=split_seed, shuffle=True)\n",
    "\n",
    "hX_train_50, hX_test_50, hy_train_50, hy_test_50 = train_test_split(data_hd, targets_hd, train_size=0.5, \n",
    "                                                        random_state=split_seed, shuffle=True)\n",
    "\n",
    "hX_train_20, hX_test_80, hy_train_20, hy_test_80 = train_test_split(data_hd, targets_hd, train_size=0.2, \n",
    "                                                        random_state=split_seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f320a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier tuning\n",
    "param_grid_dt = { \n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'max_depth' : [None, 1, 10, 20, 50, 100],\n",
    "    'criterion' :['gini', 'entropy', 'log_loss'],\n",
    "    'splitter' :['best', 'random'],\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_rf = { \n",
    "    'n_estimators': [5, 10, 20, 50, 100],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'max_depth' : [None, 1, 10, 20, 50, 100],\n",
    "    'criterion' :['gini', 'entropy', 'log_loss'],\n",
    "}\n",
    "\n",
    "param_grid_logr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': [None, 'l1', 'l2']   \n",
    "}\n",
    "\n",
    "seed = 22\n",
    "\n",
    "classifier_dt = DecisionTreeClassifier(random_state=seed, class_weight=\"balanced\")\n",
    "dt = GridSearchCV(classifier_dt, param_grid_dt, cv=3)\n",
    "\n",
    "classifier_rf = RandomForestClassifier(random_state=seed, class_weight=\"balanced\")\n",
    "rf = GridSearchCV(classifier_rf, param_grid_rf, cv=3)\n",
    "\n",
    "classifier_logr = LogisticRegression(random_state=seed, solver = \"liblinear\", class_weight=\"balanced\")\n",
    "logr = GridSearchCV(classifier_logr, param_grid_logr, cv=3)\n",
    "\n",
    "#dt = DecisionTreeClassifier()\n",
    "#rf = RandomForestClassifier()\n",
    "#logr = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f0bcec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mushroom Decision Tree\n",
      "\n",
      "80 train/20 test split\n",
      "{'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'splitter': 'best'}\n",
      "0.9998461775111521\n",
      "\n",
      "\n",
      "50 train/50 test split\n",
      "{'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'splitter': 'best'}\n",
      "0.9997538158542589\n",
      "\n",
      "\n",
      "20 train/80 test split\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': None, 'splitter': 'best'}\n",
      "0.9993849938499385\n",
      "\n",
      "\n",
      "Mushroom Random Forest\n",
      "\n",
      "20 train/80 test split\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 5}\n",
      "1.0\n",
      "\n",
      "\n",
      "20 train/80 test split\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 20}\n",
      "1.0\n",
      "\n",
      "\n",
      "20 train/80 test split\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 10}\n",
      "1.0\n",
      "\n",
      "\n",
      "Mushroom Logistic Regression\n",
      "\n",
      "20 train/80 test split\n",
      "{'C': 1, 'penalty': 'l1'}\n",
      "0.9996922129886118\n",
      "\n",
      "\n",
      "20 train/80 test split\n",
      "{'C': 10, 'penalty': 'l1'}\n",
      "1.0\n",
      "\n",
      "\n",
      "20 train/80 test split\n",
      "{'C': 1, 'penalty': 'l1'}\n",
      "0.9993849938499385\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mushroom\n",
    "\n",
    "# Decision Tree\n",
    "print(\"Mushroom Decision Tree\\n\")\n",
    "dt.fit(mX_train_80, my_train_80)\n",
    "print(\"80 train/20 test split\")\n",
    "print(dt.best_params_)\n",
    "print(dt.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "dt.fit(mX_train_50, my_train_50)\n",
    "print(\"50 train/50 test split\")\n",
    "print(dt.best_params_)\n",
    "print(dt.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "dt.fit(mX_train_20, my_train_20)\n",
    "print(\"20 train/80 test split\")\n",
    "print(dt.best_params_)\n",
    "print(dt.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "# Random Forest\n",
    "print(\"Mushroom Random Forest\\n\")\n",
    "rf.fit(mX_train_80, my_train_80)\n",
    "print(\"20 train/80 test split\")\n",
    "print(rf.best_params_)\n",
    "print(rf.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "rf.fit(mX_train_50, my_train_50)\n",
    "print(\"20 train/80 test split\")\n",
    "print(rf.best_params_)\n",
    "print(rf.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "rf.fit(mX_train_20, my_train_20)\n",
    "print(\"20 train/80 test split\")\n",
    "print(rf.best_params_)\n",
    "print(rf.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"Mushroom Logistic Regression\\n\")\n",
    "logr.fit(mX_train_80, my_train_80)\n",
    "print(\"20 train/80 test split\")\n",
    "print(logr.best_params_)\n",
    "print(logr.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "logr.fit(mX_train_50, my_train_50)\n",
    "print(\"20 train/80 test split\")\n",
    "print(logr.best_params_)\n",
    "print(logr.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "logr.fit(mX_train_20, my_train_20)\n",
    "print(\"20 train/80 test split\")\n",
    "print(logr.best_params_)\n",
    "print(logr.best_score_)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "702515ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divorce Decision Tree\n",
      "\n",
      "80 train/20 test split\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'splitter': 'best'}\n",
      "0.9706924315619968\n",
      "\n",
      "\n",
      "50 train/50 test split\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'splitter': 'best'}\n",
      "0.9880952380952381\n",
      "\n",
      "\n",
      "20 train/80 test split\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': None, 'splitter': 'best'}\n",
      "0.9722222222222222\n",
      "\n",
      "\n",
      "Divorce Random Forest\n",
      "\n",
      "80 train/20 test split\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': 'log2', 'n_estimators': 5}\n",
      "0.9706924315619968\n",
      "\n",
      "\n",
      "50 train/50 test split\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': None, 'n_estimators': 5}\n",
      "0.9766009852216749\n",
      "\n",
      "\n",
      "20 train/80 test split\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': None, 'n_estimators': 5}\n",
      "1.0\n",
      "\n",
      "\n",
      "Divorce Logistic Regression\n",
      "\n",
      "80 train/20 test split\n",
      "{'C': 1, 'penalty': 'l1'}\n",
      "0.9705314009661836\n",
      "\n",
      "\n",
      "50 train/50 test split\n",
      "{'C': 0.01, 'penalty': 'l2'}\n",
      "1.0\n",
      "\n",
      "\n",
      "20 train/80 test split\n",
      "{'C': 0.1, 'penalty': 'l2'}\n",
      "1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Divorce\n",
    "\n",
    "# Decision Tree\n",
    "print(\"Divorce Decision Tree\\n\")\n",
    "dt.fit(dX_train_80, dy_train_80)\n",
    "print(\"80 train/20 test split\")\n",
    "print(dt.best_params_)\n",
    "print(dt.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "dt.fit(dX_train_50, dy_train_50)\n",
    "print(\"50 train/50 test split\")\n",
    "print(dt.best_params_)\n",
    "print(dt.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "dt.fit(dX_train_20, dy_train_20)\n",
    "print(\"20 train/80 test split\")\n",
    "print(dt.best_params_)\n",
    "print(dt.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "# Random Forest\n",
    "print(\"Divorce Random Forest\\n\")\n",
    "rf.fit(dX_train_80, dy_train_80)\n",
    "print(\"80 train/20 test split\")\n",
    "print(rf.best_params_)\n",
    "print(rf.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "rf.fit(dX_train_50, dy_train_50)\n",
    "print(\"50 train/50 test split\")\n",
    "print(rf.best_params_)\n",
    "print(rf.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "rf.fit(dX_train_20, dy_train_20)\n",
    "print(\"20 train/80 test split\")\n",
    "print(rf.best_params_)\n",
    "print(rf.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"Divorce Logistic Regression\\n\")\n",
    "logr.fit(dX_train_80, dy_train_80)\n",
    "print(\"80 train/20 test split\")\n",
    "print(logr.best_params_)\n",
    "print(logr.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "logr.fit(dX_train_50, dy_train_50)\n",
    "print(\"50 train/50 test split\")\n",
    "print(logr.best_params_)\n",
    "print(logr.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "logr.fit(dX_train_20, dy_train_20)\n",
    "print(\"20 train/80 test split\")\n",
    "print(logr.best_params_)\n",
    "print(logr.best_score_)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b02f441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart Disease Decision Tree\n",
      "\n",
      "80 train/20 test split\n",
      "{'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'splitter': 'random'}\n",
      "0.7679324894514767\n",
      "\n",
      "\n",
      "50 train/50 test split\n",
      "{'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'splitter': 'random'}\n",
      "0.7903401360544217\n",
      "\n",
      "\n",
      "20 train/80 test split\n",
      "{'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'splitter': 'random'}\n",
      "0.8991228070175438\n",
      "\n",
      "\n",
      "Head Disease Random Forest\n",
      "\n",
      "80 train/20 test split\n",
      "{'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.818565400843882\n",
      "\n",
      "\n",
      "50 train/50 test split\n",
      "{'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 10}\n",
      "0.8448979591836734\n",
      "\n",
      "\n",
      "20 train/80 test split\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 50}\n",
      "0.9333333333333332\n",
      "\n",
      "\n",
      "Heart Disease Logistic Regression\n",
      "\n",
      "80 train/20 test split\n",
      "{'C': 1, 'penalty': 'l1'}\n",
      "0.8354430379746836\n",
      "\n",
      "\n",
      "50 train/50 test split\n",
      "{'C': 1, 'penalty': 'l1'}\n",
      "0.8308843537414967\n",
      "\n",
      "\n",
      "20 train/80 test split\n",
      "{'C': 1, 'penalty': 'l2'}\n",
      "0.8666666666666667\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Heart Disease\n",
    "\n",
    "# Decision Tree\n",
    "print(\"Heart Disease Decision Tree\\n\")\n",
    "dt.fit(hX_train_80, hy_train_80)\n",
    "print(\"80 train/20 test split\")\n",
    "print(dt.best_params_)\n",
    "print(dt.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "dt.fit(hX_train_50, hy_train_50)\n",
    "print(\"50 train/50 test split\")\n",
    "print(dt.best_params_)\n",
    "print(dt.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "dt.fit(hX_train_20, hy_train_20)\n",
    "print(\"20 train/80 test split\")\n",
    "print(dt.best_params_)\n",
    "print(dt.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "# Random Forest\n",
    "print(\"Head Disease Random Forest\\n\")\n",
    "rf.fit(hX_train_80, hy_train_80)\n",
    "print(\"80 train/20 test split\")\n",
    "print(rf.best_params_)\n",
    "print(rf.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "rf.fit(hX_train_50, hy_train_50)\n",
    "print(\"50 train/50 test split\")\n",
    "print(rf.best_params_)\n",
    "print(rf.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "rf.fit(hX_train_20, hy_train_20)\n",
    "print(\"20 train/80 test split\")\n",
    "print(rf.best_params_)\n",
    "print(rf.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"Heart Disease Logistic Regression\\n\")\n",
    "logr.fit(hX_train_80, hy_train_80)\n",
    "print(\"80 train/20 test split\")\n",
    "print(logr.best_params_)\n",
    "print(logr.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "logr.fit(hX_train_50, hy_train_50)\n",
    "print(\"50 train/50 test split\")\n",
    "print(logr.best_params_)\n",
    "print(logr.best_score_)\n",
    "print('\\n')\n",
    "\n",
    "logr.fit(hX_train_20, hy_train_20)\n",
    "print(\"20 train/80 test split\")\n",
    "print(logr.best_params_)\n",
    "print(logr.best_score_)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cde610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c22251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
